{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import polar as pl\n",
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "shapefile_path = \"app/map/taxi_zones.shp\"\n",
    "\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "# Define\n",
    "source_crs = gdf.crs  # CRS of the shapefile\n",
    "target_crs = 'EPSG:4326'  # WGS84 - lat/lon CRS\n",
    "transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "gdf['longitude'] = gdf.geometry.centroid.x\n",
    "gdf['latitude'] = gdf.geometry.centroid.y\n",
    "gdf['longitude'], gdf['latitude'] = transformer.transform(gdf['longitude'], gdf['latitude'])\n",
    "\n",
    "df = pd.DataFrame(gdf[['LocationID', 'longitude', 'latitude']])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 15:31:21 WARN Utils: Your hostname, TrungUbun resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface enp3s0)\n",
      "23/12/19 15:31:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/19 15:31:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/19 15:31:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "            SparkSession.builder.master(\"local[*]\")\n",
    "            .appName(\"SparkByExamples.com\")\n",
    "            .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .options(header=True, inferSchema=False)\n",
    "        .load(\"/home/trung/Downloads/gold_dropoff\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+\n",
      "|   Dropoff_datetime|DOLocationID|           DropOffID|\n",
      "+-------------------+------------+--------------------+\n",
      "|2023-01-22 07:49:01|        75.0|Y2023012225769803776|\n",
      "|2023-01-22 07:56:41|       236.0|Y2023012225769803777|\n",
      "|2023-01-22 07:54:23|       143.0|Y2023012225769803778|\n",
      "|2023-01-22 07:56:39|        42.0|Y2023012225769803779|\n",
      "|2023-01-22 07:31:54|        79.0|Y2023012225769803780|\n",
      "|2023-01-22 07:51:12|       144.0|Y2023012225769803781|\n",
      "|2023-01-22 07:06:24|       151.0|Y2023012225769803782|\n",
      "|2023-01-22 07:56:49|        48.0|Y2023012225769803783|\n",
      "|2023-01-22 07:42:28|       262.0|Y2023012225769803784|\n",
      "|2023-01-22 07:52:45|       158.0|Y2023012225769803785|\n",
      "|2023-01-22 07:12:28|       246.0|Y2023012225769803786|\n",
      "|2023-01-22 07:52:17|       143.0|Y2023012225769803787|\n",
      "|2023-01-22 08:04:53|        90.0|Y2023012225769803788|\n",
      "|2023-01-22 07:27:46|        50.0|Y2023012225769803789|\n",
      "|2023-01-22 07:25:06|       151.0|Y2023012225769803790|\n",
      "|2023-01-22 08:27:55|        36.0|Y2023012225769803791|\n",
      "|2023-01-22 07:41:13|       249.0|Y2023012225769803792|\n",
      "|2023-01-22 07:23:02|       142.0|Y2023012225769803793|\n",
      "|2023-01-22 08:04:12|       125.0|Y2023012225769803794|\n",
      "|2023-01-22 07:47:43|       249.0|Y2023012225769803795|\n",
      "+-------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .options(header=True, inferSchema=False)\n",
    "        .load(\"/home/trung/Downloads/a/green_tripdata_2023-02.parquet\")\n",
    "    )\n",
    "# count row have tpep_pickup_datetime null\n",
    "print(df.filter(df.lpep_pickup_datetime.isNull()).count())\n",
    "# count row have tpep_dropoff_datetime null\n",
    "print(df.filter(df.lpep_dropoff_datetime.isNull()).count())\n",
    "# count row have PULocationID null\n",
    "print(df.filter(df.PULocationID.isNull()).count())\n",
    "# count row have DOLocationID null\n",
    "print(df.filter(df.DOLocationID.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2913955"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1110797\n"
     ]
    }
   ],
   "source": [
    "# count row have dispatching_base_num null\n",
    "print(df.filter(df.dispatching_base_num.isNull()).count())\n",
    "# count row have affiliated_base_number null\n",
    "print(df.filter(df.Affiliated_base_number.isNull()).count())\n",
    "# count row have sr_flag null\n",
    "print(df.filter(df.SR_Flag.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4821\n",
      "0\n",
      "4821\n",
      "4826\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# count row have passenger_count null\n",
    "print(df.filter(df.passenger_count.isNull()).count())\n",
    "# count row have trip_distance null\n",
    "print(df.filter(df.trip_distance.isNull()).count())\n",
    "# count row have store_and_fwd_flag null\n",
    "print(df.filter(df.store_and_fwd_flag.isNull()).count())\n",
    "# count row have trip_type null\n",
    "print(df.filter(df.trip_type.isNull()).count())\n",
    "# count row have VendorID null\n",
    "print(df.filter(df.VendorID.isNull()).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passenger count, store_and_fwd_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "4821\n",
      "4821\n",
      "0\n",
      "0\n",
      "0\n",
      "4821\n"
     ]
    }
   ],
   "source": [
    "# count row have fare_amount null\n",
    "print(df.filter(df.fare_amount.isNull()).count())\n",
    "# count row have mta_tax null\n",
    "print(df.filter(df.mta_tax.isNull()).count())\n",
    "# count row have improvement_surcharge null\n",
    "print(df.filter(df.improvement_surcharge.isNull()).count())\n",
    "# count row have payment_type null\n",
    "print(df.filter(df.payment_type.isNull()).count())\n",
    "# count row have RatecodeID null\n",
    "print(df.filter(df.RatecodeID.isNull()).count())\n",
    "# count row have tip_amount null\n",
    "print(df.filter(df.tip_amount.isNull()).count())\n",
    "# count row have tolls_amount null\n",
    "print(df.filter(df.tolls_amount.isNull()).count())\n",
    "# count row have total_amount null\n",
    "print(df.filter(df.total_amount.isNull()).count())\n",
    "# count row have congestion_surcharge null\n",
    "print(df.filter(df.congestion_surcharge.isNull()).count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "payment_type, RatecodeID, congestion_surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17273410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2914436"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .options(header=True, inferSchema=False)\n",
    "        # .schema(trip_schema)\n",
    "        .load(\"/home/trung/Downloads/gold_pickup\")\n",
    "    )\n",
    "#print count null row in PULocationID\n",
    "print(df.count())\n",
    "df.filter(df.PULocationID.isNull()).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
